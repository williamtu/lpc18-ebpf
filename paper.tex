\documentclass[10pt,numbers,reprint,sigconf,authorversion]{lpc}

\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[font={small}]{caption}
\newcommand{\mycomment}[1]{}
\newcommand{\dfn}[1]{\textit{#1}} % For initial definitions of terms
\newcommand{\myfig}[3]
 {
 \begin{figure}
 \centerline{\includegraphics[width=#2]{graph/#1.pdf}}
 \caption{\sl \small #3}
 \label{#1:fig}
 \vspace*{-0.15in}
 \end{figure}
 }
\newcommand{\myfigcross}[3]
{
\begin{figure*}[ht]
\centerline{\includegraphics[width=#2]{graph/#1.pdf}}
\caption{\sl \small #3}
\label{#1:fig}
\vspace*{-0.15in}
\end{figure*}
}

% For marking text we're probably dropping.
\newcommand{\ignore}[1]{}

% For marking text that we might like to include in a final draft, if
% accepted to RADIO, but that will not fit within the 5-page limit.
\newcommand{\saveforlater}[1]{}

% For marking important work items.
\newcommand{\todo}[1]{\textbf{TODO: #1}}

\lstset{ %
    captionpos=b,
    frame=single
}

\begin{document}
\pdfinfo{
/Title (Bringing the Power of eBPF to Open vSwitch)
/Author (William Tu, Joe Stringer, Yifeng Sun and Yi-Hung Wei)}

\title{Bringing the Power of eBPF to Open vSwitch}
\author{William Tu}
\affiliation{%
      \institution{VMware Inc.}}
\email{u9012063@gmail.com}

\author{Joe Stringer}
\affiliation{%
      \institution{Cilium.io}}
\email{joe@cilium.io}

\author{Yifeng Sun}
\affiliation{%
      \institution{VMware Inc.}}
\email{pkusunyifeng@gmail.com}

\author{Yi-Hung Wei}
\affiliation{%
      \institution{VMware Inc.}}
\email{yihung.wei@gmail.com}

\acmDOI{}
\setcounter{secnumdepth}{0}
\setcopyright{none}

\maketitle

\begin{abstract}
eBPF is an emerging technology in Linux kernel with the goal of making Linux kernel
extensible by providing an eBPF virtual machine with safety guarantee.
Open vSwitch (OVS), is a software switch running majorly in Linux
operating systems, its fast path packet process is implemented in Linux kernel
module, openvswitch.ko.
To provide greater flexibility and extensibility to OVS datapath, in this
work, we present our design on making use of eBPF technology in OVS datapath
development with two projects: the OVS-eBPF project and the OVS-AFXDP project.
The goal of OVS-eBPF project is to re-write existing flow processing features
in openvswitch kernel datapath into eBPF program, and attaching it
to Linux TC.
On the other hand, the OVS-AFXDP project aims to by-pass the kernel
using an AF\_XDP socket and moves most of the flow processing features
into userspace.
We demonstrate the feasibility of implementing OVS datapath with the
aforementioned technologies and present the performance numbers in this paper.
\end{abstract}

\section{Introduction}\label{introduction}

eBPF, extended Berkeley Packet Filter, enables userspace applications to
customize and extend the Linux kernel's functionality. It provides
flexible platform abstractions for network functions, and is being ported
to a variety of platforms. In the Linux kernel, users can attach eBPF programs
to TC and XDP hook points as shown in Fig~\ref{ebpf-hook}. Based on this
design, we explore the possibilities of utilizing eBPF to implement OVS
datapath in threefold:
1) in-kernel flow processing by attaching eBPF programs to TC,
2) offloading a subset of flow processing to XDP (eXpress Data Path),
and 3) moving the flow processing to userspace by using AF\_XDP.


Firstly, in OVS-eBPF project~\cite{extensible_ovs}, we attach flow processing
eBPF programs to TC. We start with the most aggressive goal that we plan to
re-implement the entire features of OVS kernel datapath under net/openvswitch/*
into eBPF code. We work around a couple of eBPF limitations, for example, the
lack of TLV (Type-Length-Value) support leads us to redefine a binary
kernel-user API using a fixed-length array; without a dedicated way to execute
a packet, we create a dedicated device that attached a eBPF program to handle
packet execute logic for userspace to kernel packet transmission. Currently,
OVS-eBPF can satisfy most of the basic features for flow processing and
tunneling protocol support, and we are investigating more complicated features
such as connection tracking, NAT, (de)fragmentation, and ALG.

We can attach eBPF programs one layer below TC to XDP (eXpress Data Path).
It is a much faster layer for packet processing, but there is almost no extra
packet metadata available, and XDP only provides limited kernel helpers.
Depending on the type of flows, OVS can offload a subset of its flow processing
to XDP. However, the fact that XDP has fewer helper function support implies
that either 1) only very limited number of flows are eligible for offload,
or 2) more flow processing logic needed to be implemented in native eBPF code.
For example, it is more difficult for OVS datapath to provide tunnel support in
XDP, since lightweight tunnel kernel helpers are not available.

XDP provides another way for interacting with userspace programs, called AF\_XDP. 
AF\_XDP is a socket interface for control plane and a shared memory API for
accessing packets from userspace application. Using AF\_XDP, the OVS-AFXDP project
redirects packets to userspace, and processes the packets using OVS's full-fledged
userspace datapath implementation, dpif-netdev. In this approach, OVS-AFXDP
project treats the AF\_XDP as a fast packet-I/O channel.

This paper focuses on the OVS-eBPF and OVS-AFXDP projects.
%A rough comparison of the two project is shown in Table~\ref{proj-compare}.
The remainder of this paper is organized as follows:
In Section~\ref{sec:background}, we first provide some background information
on eBPF, XDP, and AF\_XDP.  We then present the design, implementation, and
evaluation of the OVS-eBPF and OVS-AFXDP projects in Section~\ref{sec:ebpf}
and Section~\ref{sec:af_xdp} respectively. Finally,
Section~\ref{sec:conclusion} concludes the paper and discusses the future work.

\begin{figure}
{\scriptsize
\begin{verbatim}

+-----------------------+
|    Network Stack      |
+---------^-------------+
          |
+---------|-------------+
| Traffic Control (TC)  | --> eBPF TC hook
+---------^-------------+
          |
+---------|-------------+
|     Driver (XDP)      | --> eBPF XDP hook
+---------^-------------+
          |
+---------|-------------+
|     Network card      |
+---------^-------------+
          |
          | packets

\end{verbatim}
}
\vspace{-1.0em}
\caption{eBPF program hook points in the Linux kernel.}
\label{ebpf-hook}
\vspace{-1.0em}
\end{figure}

\mycomment{
\begin{table}
\centering
\scriptsize
\begin{tabular}{|c | c | c|}
\hline
 {\bf Projects} & {\bf OVS-eBPF } & {\bf OVS-AFXDP} \\ \hline\hline
  Packet Rate & Low & High \\ \hline
  Minimal Kernel Req. & 4.10 & 4.18 \\ \hline
  Driver Support Req. & No & Yes \\ \hline
  BPF Code Size & Large & Minimal \\ \hline
  Extensibility & Good & Great \\ \hline
  \end{tabular}
\caption{\footnotesize
Comparison of the OVS-eBPF project and OVS-AFXDP project.
}
\label{proj-compare}
\end{table}
}
\section{Background}
\label{sec:background}

\subsection{OVS Forwarding Model}
OVS is widely used in virtualized data center environments as a software
switching layer inside various operating systems, including FreeBSD,
Windows Hyper-V, Solaris and Linux. As shown in
Figure~\ref{ovsintro}, the architecture of OVS consists of two major
components: a slow path and a fast path. OVS begins processing packets
in its datapath, the fast path, shortly after the packet is received by the NIC in the host OS.
The OVS datapath first performs packet parsing to extract relevant protocol
headers from the packet and stores it locally in a manner that is efficient for
performing lookups (flow key), then it uses this information to look into the
match/action cache (flow table) and determines what needs to be done for this
packet. If there is no match in the flow table, the datapath passes the packet
from the kernel up to the slow path, \verb+ovs-vswitchd+, which maintains the
full determination of what needs to be executed to modify and forward
the packet correctly.  This process is called packet {\em upcall} and
usually happens at the first packet of a flow seen by the OVS datapath.
If the packet matches in this flow table, then the OVS datapath executes its
corresponding actions from the flow table lookup result and updates its flow
statistics.

In this model, the \verb+ovs-vswitchd+ determines how the packet should be handled,
and passes the information to the datapath inside the kernel using a
Linux generic netlink interface.  Over the years the OVS datapath features evolved.
The initial OVS datapath used a microflow cache for its flow table,
essentially caching exact-match entries for each
transport layer connection's forwarding decision.  And in later versions,
two layers of caching were used: a microflow cache and a megaflow cache,
which caches forwarding decisions for traffic aggregates beyond individual
connections.  In recent versions of OVS, datapath implementations include
features such as connection tracking, stateful network address translation, and
support for layer 3 tunneling protocols.

\begin{figure}
{\scriptsize
\begin{verbatim}
              
                +-------------------+
 slow path      |    ovs-vswitchd   |
                +-----^-------------+
                      |       |flow installation
----------------------|-------|-----------------
 fast path     upcall |       | 
 (datapath)  +----------------v---------------+ 
  packets--> | parse, lookup match, actions   |  --> output
             |        & action table          |
             +--------------------------------+
\end{verbatim}
}
\vspace{-1.0em}
\caption{
The forwarding plane of OVS consists of two components;
ovs-vswitchd handles the complexity of the OpenFlow protocol, and the
datapath acts as a caching layer to optimize the performance.  A flow
missed by the match/action table in the datapath triggers an {\em
upcall}, which forwards the information to ovs-vswitchd.  ovs-vswitchd
installs an appropriate flow entry into the datapath's match/action table.}
\label{ovsintro}
\vspace{-1.0em}
\end{figure}

\subsection{eBPF Basics}\label{sec:ebpf}
Berkeley Packet Filter, BPF, is an instruction set architecture
proposed by Steven McCanne and Van Jacobson in 1993~\cite{cbpf}.  
BPF was designed as a generic packet filtering solution and is widely
used by every network operator today, through the well-known tcpdump/wireshark
applications. A BPF interpreter is attached early in the packet receive call
chain, and it executes a BPF program as a list of instructions.
A BPF program typically parses a packet and decides
whether to pass the packet to a userspace socket.  With its simple architecture
and early filtering decision logic, it can execute this logic efficiently.

%P2 from cBPF to eBPF
For the past few years, the Linux kernel community has improved the traditional
BPF (now renamed to classic BPF, cBPF) interpreter inside the kernel with
additional instructions, known as extended BPF (eBPF). eBPF was introduced with
the purpose of broadening the programmability of the Linux kernel. Within the
kernel, eBPF instructions run in a virtual machine environment. The virtual
machine provides a few registers, stack space, program counter, and a way to
interact with the rest of the kernel through a mechanism called helper
functions.  Similar to cBPF, eBPF operates in an event-driven model on a
particular hook point; each hook point has its own execution {\em context} and
execution at the hook point only starts when a particular type of event fires.
A BPF program is written against a specific context. For example, a BPF program
attached to a raw socket interface has a context which includes the packet, and
the program is only triggered to run when there is an incoming packet to the
raw socket.

% eBPF map, helper, tail call and syscall
%helper
The eBPF virtual machine provides a completely isolated environment for its
bytecode running inside; in other words, it cannot arbitrarily call other
kernel functions or access into memory outside its own environment. To interact
with the outside world, the eBPF architecture white-lists a set of helper
functions that a BPF program can call, depending on the {\em context} of the
BPF program.  For example, a BPF program attached to raw socket in a packet
context could invoke VLAN push or pop related helper functions, while a BPF
program with a kernel tracing context could not. %The complete list of helper
%functions can be found at \verb+linux/include/uapi/linux/bpf.h+.

%map
To store and share state, eBPF provides a mechanism to interact with a variety
of key/value stores, called \textit{maps}. eBPF maps reside in the kernel, and can be
shared and accessed from eBPF programs and userspace applications. eBPF
programs can access maps through helper functions, while userspace applications
can access maps through BPF system calls. There are a variety of map types for
different use cases, such as hash tables or arrays. These are created by a
userspace program and may be shared between multiple eBPF programs running in
any hook point.

%tail call
Finally, eBPF tail call~\cite{tailcall} is a mechanism allowing one eBPF program to trigger execution of 
another eBPF program, providing users the flexibility of composing a chain of
eBPF programs with each one focusing on particular features.  Unlike a
traditional function call, this tail call mechanism calls another program
without returning back to the caller's program. The tail call reuses the
caller's stack frame, which allows the eBPF implementation to minimize call
overhead and simplifies verification of eBPF programs.
% end of bpf intro

\subsection{XDP: eXpress Data Path}
\myfig{tcebpf}{2.6in}{The workflow of TC and XDP eBPF development process and its
packet flow.  The eBPF program compiled by LLVM+clang is loaded into the kernel
using iproute. The kernel runs the program through a verification stage, and
subsequently attaches the program to the TC/XDP ingress hook point.  Once
successfully loaded, an incoming packet received by XDP/TC ingress will execute the
eBPF program.}

There are several hook points where eBPF programs can be attached in recent
Linux kernels. XDP~\cite{xdp,xdp2,xdp-conext18} is another eBPF program hook point where
its attachment point is at the lowest level of the network stack.
XDP demonstrates high performance that closed to the line rate of the device,
since the eBPF programs attached to XDP hook point are triggered immediately
in the network device driver's packet receiving code path. For the same reason,
eBPF program in XDP can only access the packet data and a few metadata.
XDP supports accessing to eBPF maps and tail calls, but much less number of
helper functions is available compared to the TC hook point.

Figure~\ref{tcebpf:fig}
shows the typical workflow for installing an eBPF program to the TC/XDP hook point,
and how packets trigger eBPF execution.  Clang and LLVM takes a program
written in C and compiles it down to the eBPF instruction set, then emits an
ELF file that contains eBPF instructions.  An eBPF loader, such as iproute,
takes the ELF file, parses the programs and maps information from it and
issues BPF syscalls to load the program.  If the program passes the BPF verifier,
then it is attached to the hook point (in this case, TC/XDP), and
subsequent packets through the TC/XDP ingress hook will trigger execution of the
eBPF programs.

\subsection{AF\_XDP Socket}
% afxdp
AF\_XDP is a new Linux address family that aims for high packet I/O
performance. It enables another way for a userspace program to receive
packets from kernel through the socket API. For example, currently, creating
a socket with address family AF\_PACKET, userspace programs can receive/send
the raw packets
at the device driver layer.  Although the AF\_PACKET family has been using
in many places such as tcpdump, its performance does not catch up with the
recent high speed network devices, such as 40G/100G NICs.
Performance evaluation~\cite{af_packet_v4,af_packet_performance} of AF\_PACKET
shows less than 2 million packets per second using single core.

AF\_XDP was proposed and upstreamed to Linux kernel since 4.18~\cite{af_xdp_lwn}.
The core idea behind the AF\_XDP is to leverage the XDP eBPF program's
early access to the raw packet, and provide a high speed channel from the XDP
directly to a userspace socket interface. In other word, AF\_XDP socket family
connects the XDP packet receiving/sending path to the userspace, by-passing the
rest of the Linux networking stacks.
An AF\_XDP socket, called XSK, is created by using the normal socket() system
call. Unlike AF\_PACKET which uses the send() and receive() syscalls with packet
buffer as parameter, XSK introduces two rings in userspace: the Rx ring and the
Tx ring. These two rings are per-XSK data structure that the userspace program
needs to properly configure and maintain in order to receive and send packets.
Moreover, to provide zerocopy support for AF\_XDP, all the packet data buffers
used in Tx/Rx rings are allocated from a specific memory region called {\em umem}
which consists of a number of fixed size memory chunks.
Two rings, the Fill ring and the Completion ring, are associated with a umem,
and a umem can be shared between multiple XSKs.
Each element in the Rx/Tx ring or in Fill/Completion ring is a descriptor that
contains an address that points to a chunk in umem. The address is not system's
virtual or physical address but simply an offset within the umem memory
region.

For example, to receive packets from XSK, firstly, the userspace program firstly
pushes a set of descriptors pointing to empty packet buffer into the Fill ring.
When a packet
arrives, kernel pops descriptors from the Fill ring, fills in the data into the
memory chunks pointed by the descriptors, and pushes the descriptors back to
the Rx ring.
The userspace program then checks the Rx ring, fetches the packet data from the
descriptors, and refills the empty buffer back to the Fill ring structure, so
that kernel can fill in new incoming packets later on.
For sending packets, the userspace program pushes a set of descriptors that
point to the packet buffers to the Tx ring, then issues sendmsg() system call.
Kernel consumes packet buffers asscoaited with the Tx ring, and pushes
transmitted descriptors to the Tx ring.
The userspace program then checks the Completion ring to determine whether the
packets have been sent.
In summary, XSK users need to properly program following four rings:
\begin{itemize}
\item Fill ring: for users to fill umem addresses to kernel for receiving packets.
\item Rx ring: for users to access received packets.
\item Tx ring: for users to place packets needed to be sent.
\item Completion ring: for users to check if packets are sent.
\end{itemize}
Unlike AF\_PACKET which is bound to entire netdev, the binding of XSK is more
fine-grained. XSK is bound to a specific queue on a device, so only the
traffic sent to the queue shows up in the XSK.

\section{OVS eBPF Datapath}
\label{sec:ebpf}
\myfigcross{ovsebpf}{5.5in}{The overall architecture of OVS eBPF datapath
consists of multiple eBPF programs which are tail-called dynamically, maps
which are shared between eBPF programs and userspace applications, and
ovs-vswitchd as the management plane for all components.}

\subsection{eBPF Configuration and Maps}
The OVS eBPF datapath consists of one ELF-formatted object file which provides
the full functionality of an OVS datapath. This object defines
a set of maps and a set of eBPF programs which implement a subset of
the datapath functionality.  To bootstrap, we load the eBPF program into
the kernel using \verb+iproute+. One of the programs is marked
within the ELF file to indicate that it should be
attached to the hook point; the other programs are only executed via tail
calls rooted in the eBPF hook point.  The ELF file also defines multiple persistent eBPF maps, which are
pinned to the BPF filesystem ~\cite{bpfpmap}
for sharing between different eBPF programs and \verb+ovs-vswitchd+.
The OVS datapath requires the following eBPF maps:
\begin{itemize}
\item Flow key. This is the internal representation of the protocol headers
    and metadata for the current packet being processed.
\item Flow Table. This is a hash table whose key is the 32-bit hash value
of the flow key, from both packet and its metadata,
and value equals an array of actions to execute upon the flow.
\item Stats Table. This is similar to the flow table, but rather than holding
an array of actions to execute for the packet, it contains packet and byte
statistics for the flow.
\item Perf Ring Buffer~\cite{ebpfperf,ebpfperf2}. The perf event map
allows an eBPF program to put user-defined data into a ring buffer which
may be read from a userspace program. \verb+ovs-vswitchd+ memory maps
this ring buffer to read packets and metadata from the eBPF program for
flow miss upcall processing.
\item Program Array. This map allows eBPF programs to tail call other eBPF
programs. When the BPF loader inserts eBPF programs into the kernel, it assigns
unique indexes for each program and stores these into the map.  At run time,
an eBPF program will tail call another program by referring to an index within
this map.
\end{itemize}

% locking and synchronization
The OVS eBPF program is triggered by the TC ingress hook associated with a
particular network device. Multiple instances of the same eBPF program
may be triggered simultaneously on different cores which are receiving traffic
from different network devices.  The eBPF maps, unless specified, have a single
instance across all cores.  Access to map entries are protected by the kernel
RCU~\cite{rcu} mechanism which makes it safe to read concurrently.  However,
there are no built-in mechanisms to protect writers to the maps.
For the flow table map, OVS avoids the race by ensuring that
only \verb+ovs-vswitchd+ inserts or removes elements from the map from a single
thread.  For flow statistics, atomic operations are used to avoid race
conditions.  Other maps such as the flow key perf ring buffer maps use
per-cpu instances to manage synchronization.

\subsection{Header Parsing and Lookup}
\label{parsing}
When a packet arrives on the TC ingress hook, the OVS eBPF datapath begins
executing a series of programs, beginning with the parser/lookup program
as shown in Figure~\ref{ovsebpf:fig}.  The eBPF parser program
consists of two components; standard protocol parsing and Linux-specific
metadata parsing.  The protocol parsing is executed directly on the packet
bytes based on standardized protocols, while the platform-specific metadata
parsing must occur on the {\em context} provided by the eBPF environment.

The resulting code will assemble the protocol headers and metadata,
collectively known as the flow key. This flow key is then used to look for an
entry in the flow table map, to get an array of actions to execute. If there is
no entry in the flow table map, then the packet and the flow key will be
written to the perf event map for further processing by \verb+ovs-vswitchd+.

\subsection{Action Execution}
\label{execution}
When a lookup is successful the eBPF gets a list of actions to be executed,
such as outputting the packet to a certain port, or pushing a VLAN tag. The
list of actions is configured in \verb+ovs-vswitchd+ and may be a variable
length depending on the desired network processing behaviour. For example, an
L2 switch doing unknown broadcast sends packet to all its current ports.  The
OVS datapath's actions is derived from the OpenFlow action specification and
the OVSDB schema for \verb+ovs-vswitchd+.

One might expect to intuitively write an eBPF program to iterate across the
list of actions to execute, with each iteration of the loop dealing with a
particular action in the list.  Unfortunately, this type of iteration implies
dynamic loops, which are restricted within the eBPF forwarding model. Moreover,
the variable number of actions also implies that there is no way to guarantee
the bounded program size, which is limited to 4,096 eBPF bytecode instructions.

To solve these challenges, we first break each type of action logic into an
independent eBPF program and tail call from one eBPF action program to
another, as shown in Figure~\ref{ovsebpf:fig}.  This alleviates
the problem from having 4k instructions for the {\em entire} action list to 4k
instructions {\em per action}.  Our proof-of-concept implementation shows that
this limitation is sufficient for all existing actions.  Additionally, the
design allows each action to be implemented and verified independently.
Second, to solve the dynamic looping problem, we convert the variable length
list of actions into a fixed length 32-element array.  As a result,
flow table lookup always returns an array of 32 actions to be executed,
and the LLVM compiler unrolls the loop to pass the BPF verifier.
If a matching flow has less than 32 actions to execute, the rest of the
actions is no-op, and we short-cut the action execution to the deparser.
If a matching flow has more than 32 actions, then the eBPF datapath delegates
the execution to userspace datapath, i.e., the slow path. For many use cases,
this is sufficient; there is room to further optimize this path in the future
if common cases require more than 32 actions.

Each action also requires certain action-specific metadata to execute.
For example, an output action would require an {\em ifindex} to
indicate the output interface, while a push\_vlan action needs the {\em VLAN ID}
to be inserted into the VLAN tag.  To accommodate this, the array element not
only contains the action to execute, but also the metadata required by the
particular action. The size of each element must be big enough to hold the
metadata for any action, as it is a fixed sized array. Future work may
relax this requirement.

Each eBPF program executes in an isolated environment. As a consequence
of breaking the action list into individual eBPF action programs, some state needs to be
transferred between the current eBPF program and the next.
The state to transfer from one eBPF action program to another includes
(1) The flow key, so that the currently executing eBPF action program can
lookup the flow table, and (2) The action index in the actions list, so that
the eBPF action program knows it is executing the $N$th element in the list,
and at the end of its processing, to tail-call the $n+1$th action program.
Currently eBPF could use either per-CPU maps as scratch buffers or the
context's packet control block (\verb+cb+) to pass data between programs. In our
design, we use one 32-bit value in the \verb+cb+ to keep the action
index, and per-cpu maps to save the flow key.
%As a result, there is additional overhead of map lookup per action.

% an example to illustrate
Figure~\ref{ovsebpf:fig} also demonstrates an example eBPF execution of a
packet forwarded to port 2 as well as port mirroring to a VLAN port with VLAN ID
100 at port 3.  Once the packet is parsed,
the flow table lookup returns an action list of output:2, push\_vlan:100,
output:3.  At the end of the parser+lookup program, it tail calls the eBPF
output program as the first step to kick start action execution.
The execution of this output program {\em overwrites} the caller's stack,
so it has to look up the flow table map to retrieve the flow key,
and execute the output action.  Once done, the output action increments the
action index from 0 to 1, saves it in \verb+cb+, and tail-calls the next action
program, which is push\_vlan.  The push\_vlan eBPF program again 
looks up the flow table, and fetches the action metadata at index 1
and executes the push VLAN action using a BPF helper function.
The third output action follows the same procedure and finally sends the
packet out to port 3.
\saveforlater{Appendix A shows the eBPF helper functions used in
each eBPF action program implementation.}

\subsection{Flow Miss Upcall and Installation}
One of the important tasks of the OVS datapath is to pass any packet that
misses its flow table to the slow path to get instructions for
further processing.  In the existing OVS {\em kernel} datapath implementation,
the missed packet is sent to \verb+ovs-vswitchd+, which processes the packet,
inserts a flow into the flow table, and re-injects the packet into the kernel
datapath using the Linux generic netlink interface.  For the eBPF datapath, this
design implies two requirements: (1) a way for the eBPF program to communicate
with userspace, and (2) a mechanism for the userspace to re-insert packet into
the eBPF program.

% upcall: how ebpf work 
To address the first requirement, we use the support for Linux perf ring
buffers and the \verb+skb_event_output()+ eBPF helper function~\cite{ebpfperf}
which allows the eBPF program to pass data to the userspace through the Linux perf
ring buffer~\cite{ebpfperf}.  During miss upcall processing, the eBPF program
will insert the full packet and the current flow key into the perf ring buffer.
To receive the data from userspace, \verb+ovs-vswitchd+ runs a thread which
maps the ring buffer to its virtual memory using mmap system call, and polls
the buffer using the poll system call. If there is incoming data from the ring buffer, the
thread is woken up, it reads from the buffer and processes the packet and
metadata. The result of this processing will be inserted into the flow table
map.
% down call: how to re-insert packet
To address the second requirement, we construct a dedicated Linux TAP device
which also has the OVS eBPF datapath program attached to it.
\verb+ovs-vswitchd+ sends the missed packet using an AF\_PACKET socket,
triggering the underlying eBPF program for further processing.
This program is very similar to the
previously-used parser+lookup program, with minor changes. Specifically, this
packet was originally received on one device, however when \verb+ovs-vswitchd+
sends the packet on the TAP device, the eBPF program is triggered for the TAP
device instead. So, the platform metadata for incoming port misidentifies the
source as the TAP device. To ensure that the packet lookup occurs correctly,
\verb+ovs-vswitchd+ prepends the port number to the beginning of the packet
data, then when the eBPF program for the dedicated TAP device processes the
packet, it will read this port number into the metadata, then strip this port
from the packet. The resulting packet is identical to the originally-received
packet, and now the metadata will match the metadata originally generated by
the parser+lookup the first time the packet was received. The rest of the
lookup, actions execution, and deparser is then executed as per the description
in the previous sections.

\subsection{Evaluation}\label{evaluation}
To quantify the performance of the OVS eBPF datapath, we measure the packet
forwarding rate in millions of packets per second (Mpps), using 64-byte
packets under different forwarding scenarios. The hardware testbed
consists of two Intel Xeon E5 2650 servers, each with an Intel 10GbE
X540-AT2 dual port NIC, with the two ports of the Intel NIC on one server
connected to the two ports on the identical NIC on the other server. The
OVS eBPF datapath is installed on one server, acting as a bridge to
forward packets from one port on the NIC to the other, and vice-versa.
The other server acts as packet generating host, which runs DPDK
packet-gen sending at the maximum packet rate of
$14.88$Mpps. This server sends to one port to the target server, and
receives the forwarded packets on the other port. All experiments use
only one CPU core running Linux kernel 4.9-rc6.
\begin{table}
\centering
\small
\begin{tabular}{|c | c | c| c|}
\hline
 {\bf Linux Bridge} & {\bf OVS Kernel } & {\bf Linux TC} & {\bf OVS eBPF}\\ \hline
 1.6 Mpps & 1.4 Mpps & 1.9 Mpps & 1.12 Mpps\\ \hline
\end{tabular}
\caption{\footnotesize
Baseline port-to-port forwarding rate using existing Linux subsystems and
OVS kernel and eBPF.
}
\label{table:baseperf}
\end{table}

\textbf{Baseline Forwarding Performance.}
We start by conducting a simple port-to-port packet forwarding experiment,
i.e., receiving packets from one port and outputting to the other,
using the Linux native bridge, Linux TC, and OVS kernel datapath, as shown in
Table~\ref{table:baseperf}.
The native Linux bridge is a simple L2 mac-learning switch with no
programmability, showing the forwarding rate of $1.6$ Mpps.
The OVS kernel module, which does additional flow key extraction,
shows a slower forwarding rate of $1.4$ Mpps.
For forwarding packets using TC, we have an eBPF program loaded
into TC, with the program only having one BPF helper function call, the
bpf\_skb\_redirect, that redirects the incoming packet from one interface
to another.  Since TC accesses the incoming packets closest to the driver
layer, it shows the highest performance of $1.9$ Mpps.
Finally, we measure our proposed OVS eBPF forwarding speed,
with incoming packets traversing through an eBPF parser+lookup program,
an output action program, and the deparser program.
Since the OVS eBPF is implemented based on tail calling these
additional eBPF programs at TC, we observe the overhead of $0.78$ Mpps,
a reduction from $1.9$ Mpps to $1.12$ Mpps.

\textbf{Forwarding with action execution.}
To further investigate the overhead,
we program one additional action executed before the packet is
forwarded to the other port.  Since the Linux bridge has no
programmability, we only compare the OVS kernel datapath with the OVS eBPF
datapath.  Table~\ref{table:ebpfperf} shows forwarding
packets while executing the additional action type: hash, push\_vlan,
set\_dst\_mac, and set\_gre\_tunnel, with the OVS kernel and eBPF datapaths.
The forwarding performance of these results is bounded by the CPU cost.
In the case of the hash and vlan actions, the NIC allows the processing
to be offloaded from the CPU, resulting in minimal performance overhead for
executing these actions.  As a result, the forwarding rate exhibits little
to no overhead above the baseline.
Moreover, for the hash and push\_vlan actions, due to the NIC offloading of
actions processing, the datapath does not need to execute any modification to
the packet contents, so the deparser program execution may be optimized out.

To ensure invoking the deparser, the third experiment, set\_dst\_mac, involves
altering packet data, specifically to modify the destination MAC address. In
this case, the deparser writes stored metadata from actions processing back to
the packet. The performance in this experiment drops to $0.84$ Mpps. The drop
of $0.28$ Mpps from baseline is the result of the deparser writing all known
packet header metadata back to the packet. With a more intelligent deparser
implementation, and by using recent improvements to the eBPF
API~\cite{dpa1,dpa2}, the performance gap compared with the native kernel
implementation is expected to shrink.

%The Set DST\_MAC result of $0.84$ Mpps makes the packet modification using the P4-generated deparser.
Finally, the set\_get\_tunnel action experiment yielded $0.48$ Mpps, which
represents the additional cost of tunnelling traffic; such traffic must
traverse the Linux network stack twice, once for overlay traffic and
once for underlay traffic. The eBPF result is also more expensive than
the existing Linux implementation, but with a narrower performance gap
than the earlier experiments. The majority of processing in this case occurs
outside of OVS, so the overhead of the eBPF datapath has less effect.

\begin{table}
\centering
\small
\begin{tabular}{|c | p{1.4cm} | p{1.4cm}| c|}
\hline
{\bf Action} & {\bf eBPF DP} & {\bf Kernel DP} & {\bf Overhead}\\ \hline 
hash & 1.12 & 1.34 & 16\%\\ \hline
push\_vlan & 1.11 & 1.32 & 15\% \\ \hline
set\_dst\_mac & 0.84 &  1.28 & 34\% \\ \hline
%set\_dst\_mac (no parser) & 1.14 & -- & --\\ \hline
set\_gre\_tunnel & 0.48 & 0.57 & 8\%\\ \hline 
\end{tabular}
\caption{\footnotesize Comparison of single core forwarding rate in Mpps
with eBPF and Kernel Datapath, with the additional
action executed before forwarding the packet to another port.}
\label{table:ebpfperf}
\end{table}

\section{Userspace OVS with AF\_XDP}
\label{sec:af_xdp}
\begin{figure}
{\scriptsize
\begin{verbatim}
              |   +-------------------+
              |   |    ovs-vswitchd   |<-->ovsdb-server
              |   +-------------------+
              |   |      ofproto      |<-->OpenFlow controllers
              |   +--------+-+--------+ 
              |   | netdev | |ofproto-|
    userspace |   +--------+ |  dpif  |
              |   | AF_XDP | +--------+
              |   | netdev | |  dpif  |
              |   +---||---+ +--------+
              |       ||     |  dpif- |
              |       ||     | netdev |
              |_      ||     +--------+  
                      ||         
               _  +---||-----+--------+
              |   | XDP program +     |
 Linux kernel |   |   eBPF map        |
              |_  +--------||---------+
                           ||
                        physical
                           NIC
\end{verbatim}
}
\vspace{-1.0em}
\caption{OVS Architecture with AF\_XDP}
\label{ovsafxdp}
\vspace{-1.0em}
\end{figure}

\subsection{Datapath and Netdev Interface}
OVS provides a userspace datapath interface (dpif) implementation, called
dpif-netdev. The dpif-netdev userspace datapath receives and transmits
packets from its userspace interface.  One major use case of dpif-netdev
is OVS-DPDK, where the packet reception and transmission
are all conducted in DPDK's userspace
library. The dpif-netdev is designed to be agnostic to how the network
device accessing the packets, by an abstraction layer called netdev.
Therefore, packets can come from DPDK packet I/O library,
a Linux AF\_PACKET socket API, or AF\_XDP socket interface,
as long as each mechanism implements its own netdev interface.
Once dpif-netdev receives a packet, it follows the same mechanism performing
parse, lookup the flow table, and apply actions to the packet.

Figure~\ref{ovsafxdp} shows the architecture of userspace OVS with AF\_XDP.
We implement a new netdev type for AF\_XDP, which receives and transmits
packets using the XSK.  We insert a XDP program and a eBPF map which interacts
with XDP program to forward packets to the AF\_XDP socket.
Once the AF\_XDP netdev receives a packet, it passes the packet to the
dpif-netdev for packet processing.

\subsection{AF\_XDP netdev configuration}
When users attach a AF\_XDP netdev to an OVS bridge, for example, by
issuing the following commands:
{\small
\begin{verbatim}
    ovs-vsctl add-br br0
    ovs-vsctl add-port br0 eth0 -- \
        set int eth0 type="afxdp"
\end{verbatim}
}
ovs-vswitchd does the following steps to bring up the
AF\_XDP netdev:
\begin{enumerate}
\item Attach a XDP program to the netdev's queue:
OVS attaches a simple and fixed XDP program to each netdev's queue.
The program only consists of a few lines of code, which receives the
packets and redirects them to XSK by calling the \texttt{bpf\_redirect\_map()}
helper function.

\item Create a AF\_XDP socket:
Call socket() syscall to create a XSK,
set up its Rx and Tx ring buffer, allocate a umem region,
and set up Fill/Completion ring of the umem.

\item Load and configure the XSK eBPF map:
The XSK eBPF maps consists of key value pairs, where key is an u32 index and
value is the file descriptor of the XSK.  ovs-vswitchd programs an entry to
the map with the key as queue id and the file descriptor, fd, of the XSK as
its value.
Therefore, the XDP program calling \texttt{bpf\_redirect\_map} will derive
the corresponding XSK with the queue id.

\item Populate the umem Fill ring: Get a couple of umem elements and place into Fill ring.
\end{enumerate}
Finally, when a AF\_XDP netdev is detached or closed by user,
ovs-vswitchd closes the XSK socket, free the umem memory region, and
unload the eBPF program and map.

\subsection{umem memory management}
In order to properly program Rx/Tx/Fill/Completion rings, we implement a
umem memory management layer, call umempool. It is a data structure
maintaining the unused/available elements in umem with GET and PUT access
APIs. We will demonstrate the use case of umempool with packet reception and
transmission in the following subsections.

\subsubsection{Packet Reception}
\begin{figure}
{\scriptsize
\begin{verbatim}
umem: total 8 elements, each has 2KB chunk size
                 +-------------------------------+
                 |   |   |   |   |   |   |   |   |
                 +-------------------------------+
            addr:  1   2   3   4   5   6   7  8 
(1) Initial Stat
           -------------------          umempool
Fill r   ...| 1 | 2 | 3 | 4 | ...       {5, 6, 7, 8}
           -------------------
           -------------------
Rx ring  ...|   |   |   |   | ...
           -------------------
(2) 4 Packets Arrive
           -------------------          umempool
Fill r   ...|   |   |   |   | ...       {5, 6, 7, 8}
          --------------------
           -------------------
Rx ring  ...| 1 | 2 | 3 | 4 | ...
           -------------------
(3) Get 4 free umem elemet and refill
           -------------------          umempool
Fill r   ...| 5 | 6 | 7 | 8 | ...       {} 
           -------------------
           -------------------
Rx ring  ...| 1 | 2 | 3 | 4 | ...
           -------------------
(4) Pass packets {1, 2, 3, 4} to dpif-netdev
    for parse, lookup, and actions.

(5) Recycle to umempool, goto step (2)
            +-------------------        umempool
Fill r  ...| 5 | 6 | 7 | 8 | ...       {1, 2, 3, 4}
            +------------------
           -------------------
Rx ring  ...|   |   |   |   | ...
           -------------------
\end{verbatim}
}
\vspace{-1.0em}
\caption{An example of how OVS programs the Fill ring and Rx ring when
processing incoming AF\_XDP packets.}
\label{afxdprx}
\vspace{-1.0em}
\end{figure}

Figure~\ref{afxdprx} shows how ovs-vswitchd sets up the Fill ring and
the Rx ring for receiving packets from XSK.  For simplicity, in this example,
we assume that there are only eight umem buffers, and each buffer's
size is 2KB.  Initially, at step 1, ovs-vswitchd pushes four available umem
elements into
the Fill ring and waits for incoming packets.  When there are incoming packets,
at step 2, the Fill ring's four buffer elements will be consumed and moved
to the Rx ring.  In order to keep receiving packets, ovs-vswitchd gets another
four available umem elements from the umempool, and fills into Fill ring (step 3).
Then, ovs-vswitchd creates the metadata needed for the four packet
buffer \{1, 2, 3, 4\}, i.e., struct dp\_packet and struct dp\_packet\_batch, and passes to 
the dpif-netdev layer for parse, lookup and action executions.
Finally, when ovs-vswitchd finishes processing the umem buffer, a recycle
mechanism is triggered to place this buffer back to umempool (step 5).
Step 5 makes sure that there are always available elements in Fill ring, so
that the underlying XDP program in kernel can keep processing packets 
while the userspace ovs-vswitchd is processing the previous
received packets on Rx ring.
When step 5 finishes, ovs-vswitchd goes back to step 2, waiting for
new packets.

\subsubsection{Packet Transmission}
\begin{figure}
{\scriptsize
\begin{verbatim}
 OVS sending 4 packets recevied from eth1 to eth2 by XSK.
(1) Get 4 free elements from eth2's umempool, copy packet data to them
(2) Create TX descriptors and place into Tx ring
           -------------------        eth2 umempool
Tx ring  ...| 4 | 5 | 6 | 7 | ...       {1, 2, 3, 8}
           -------------------
           -------------------
Compl r  ...|   |   |   |   | ...
           -------------------
(3) Issue sendmsg syscall to eth2's XSK
(4) Packet transmission is completed
           -------------------        eth2 umempool
Tx ring  ...|   |   |   |   | ...       {1, 2, 3, 8}
           -------------------
           -------------------
Compl r  ...| 4 | 5 | 6 | 7 | ...
           -------------------
(5) Recycle to eth2 umem
           -------------------        eth2 umempool
Tx ring  ...|   |   |   |   | ...       {1, 2, 3, 4,
           -------------------           5, 6, 7, 8}
           -------------------
Compl r  ...|   |   |   |   | ...
           -------------------
(6) Recycle the 4 packet buffers back to eth1's umempool
\end{verbatim}
}
\vspace{-1.0em}
\caption{An example of how OVS programs the COMPLETION ring and Tx ring when
processing incoming AF\_XDP packets from one netdev and sending them to another
netdev.}
\label{afxdptx}
\vspace{-1.0em}
\end{figure}

Figure~\ref{afxdptx} shows the process for sending packets to the XSK.
In this example, there is one bridge and two ports, eth1 and eth2, in OVS.
Both ports are configured with AF\_XDP support, and there is a flow,
\texttt{in\_port=eth1, actions=output:eth2} that forwards
packets received from eth1 to eth2.

Initially, assume ovs-vswitchd receives four packets from eth1's XSK.
To send packets to eth2 using eth2's XSK, ovs-vswitchd first gets four packet buffers,
\{4, 5, 6, 7\} from eth2's umempool and copies the packet data from eth1's umem
to eth2's umem at \{4, 5, 6, 7\}.  Later on, TX descriptors for the four packets are
created and placed into eth2's Tx ring (step 2).
At step 3, sendmsg syscall is issued to signal the kernel to start the transmission.
As the send msg syscall in XSK is asynchronous, ovs-vswitchd needs to poll
the COMPLETION ring to make sure that these four packets have been sent out
(step 4).
Once the four packets' descriptors show up at the COMPLETION ring, at step 5,
ovs-vswitchd recycles their umem elements back to eth2's umempool.
The original four packet buffers from eth1 are recycled back to eth1's
umempool as well.

\subsection{Performance Evaluation and Optimization}
All of our performance evaluation are conducted on a test bed consisting of
two Intel Xeon E5 2440 v2 1.9GHz servers, each with 1 CPU socket and
8 physical cores with hyperthreading enabled.
The first server, the source server, is served as a traffic generator.
It is equipped with a Netronome NFP-4000 40GbE device, and runs a customized
traffic generator that generates 64-byte single UDP flow packets
at the rate of $19Mpps$ using the DPDK library.
The second server, the target server, is equipped with an Intel 40GbE XL710
single port NIC, and it runs Linux kernel 4.19-rc4 with i40e driver supporting
the AF\_XDP zero copy mode~\cite{af_xdp_patch_i40e, af_xdp_patch_zc}.
We install OVS-AFXDP on the target server, and
we disable the Intel Spectre and Meltdown fixes~\cite{jesper_xdp_perf_drop, retpoline}
for demonstrating the best performance. In addition, we enable 1GB huge
memory page support to reduce the page fault overhead.

For all our experiments, we use a microbenchmark program, called xdpsock,
as the baseline to compare with OVS-AFXDP implementation.
xdpsock is an AF\_XDP sample program that can be configured into two modes.
In the rxdrop mode, xdpsock simply drops all incoming packets without any
packet processing. In the l2fwd mode, xdpsock forwards the packet to the
same port from where it is received.
In our testbed, we measure $19Mpps$ for xdpsock-rxdrop and $17Mpps$ for xdpsock-l2fwd.
For OVS-AFXDP, we conduct two similar experiments, with OVS's OpenFlow rule installed as
below:
\begin{itemize}
\item OVS-AFXDP rxdrop: Install a single OpenFlow rule to drop every packets,
e.g.,  \texttt{in\_port=eth1, actions=drop}.
\item OVS-AFXDP l2fwd: Install a single OpenFlow rule to forward packet to the same port
as it receives, e.g., \texttt{in\_port=eth1, actions=set\_fields:eth2->in\_port, output:eth1}.
\end{itemize}

AF\_XDP can provide the following three operation modes depending on the device
and driver support. We list the three mode from the slowest to the fastest
throughput.
1) skb mode: works on devices using generic XDP~\cite{genericxdp},
2) driver mode: works on devices with XDP support, and
3) zero-copy driver mode: works on devices with XDP\_XDP zerocopy support.
For all our experiments, we use the zero-copy driver mode since it provides the
best performance.

\subsubsection{PMD netdev}
In the beginning, our initial prototype of OVS-AFXDP only provides $0.5Mpps$
throughput for rxdrop.
We observe that when dropping/forwarding packets under OVS-AFXDP, two processes
below can easily reach 100\% CPU utilization:
\begin{itemize}
\item ovs-vswitchd: This is the process that keeps doing the send and receive steps
in Figure~\ref{afxdprx} and~\ref{afxdptx}.
\item ksoftirqd/core\_id: This is the kernel software interrupt thread handling the
incoming packets, triggering XDP program to pass packets to the XSK, and also
processing transmission.
\end{itemize}
\mycomment{Compared to xdpsock, ovs-vswitchd adds on top of it a umempool,
and packets parsing, lookup and action execution.}
With further investigation using Linux perf tools, our first improvement is
to enable OVS's Poll-Mode-Driver (PMD) mode to the AF\_XDP netdev.

In OVS's non-PMD mode, ovs-vswitchd does packet reception by putting all the
receiving netdev's file descriptors (fd)s together and invokes the poll system
call to determine if any one of the fd is ready to perform I/O.  As a result,
the polling of XSK's fd  is {\em shared} with other fds, and
we observed that the poll system call incurs high overhead.

Applying OVS's PMD netdev avoids these problems and improves the rxdrop performance
from $0.5Mpss$ to $3Mpps$. This performance gain is because of reducing userspace
and kernel space context switch overhead by omitting poll system call.
That is instead of using poll system call, we implement packet reception by using
PMD netdev for AFXDP, so that ovs-vswitchd allocate a dedicated thread for each
XSK's receive queue and the thread keeps polling the RX ring for new packets.
Currently, each round of receive polling processes a batch up to 32 packets.

\subsubsection{Memory Management and Locks}\label{memorym}
For every stage of optimizations, we use Linux perf extensively, e.g., perf stat
and perf record/report.  With the above PMD netdev optimization, we observe the
new bottleneck is the umempool APIs that we introduced.
We introduce two major umempool APIs, umem\_elem\_get(), which gets N free
element from the umempool, and umem\_elem\_put(), which places back the free
umem buffer to the umempool. We implement three different data structures for
umempool management as below.
\begin{itemize}
\item LIFO-list\_head: We reverse the first 8-byte of a umem chunk as a next
pointer that pointing to the next available element in umempool, and implement
LIFO (last in, first out) access get/put APIs. In this design, we maintain the
list\_head pointer that always points to the first available umem element,
and we get and put elements from list\_head.
\item FIFO-ptr\_ring: This design maintains an extra FIFO (first in, first out)
pointer ring that is similar to Linux kernel's ptr\_ring. The ring is an array
of elements that allocated in a continuous memory region, where each element
points to an available umem chunk. We keep track of head and tail pointer for
the FIFO-ptr\_ring. The consumer gets elements from the tail pointer,
and then the producer puts elements back to where head pointer points to.
\item LIFO-ptr\_array: This design is similar to Linux kernel's
skb\_array~\cite{skbarray} where an array of pointers are allocated
in contiguous memory region. Each element in the array points to an available
memory chunk in umem.  We keep track of a head pointer to the array,
and both consumer and producer get/put elements from the head pointer.
\end{itemize}

Initially, we allocate one umem per netdev. Since there might be multiple
queues per netdev sharing the same umem, the above three data structures
require a mutex lock to protect umempool accessing. In this design, Linux
perf reports pthread mutex lock related APIs as one of the top CPU
utilization function.
We then change our design by 1) allocating per-queue umem region and 2)
allocating one PMD thread per queue. As a result, no lock is needed because
each queue has only one thread and its own set of umem elements.

\subsubsection{Metadata Allocation}
Moving forward, Linux perf shows that the packet metadata allocation
takes a lot of CPU cycles, i.e.,  dp\_packet\_init(), dp\_packet\_set\_data().
So instead of allocating packet metadata at packet reception time, we pre-allocate
the packet metadata and implement two data structures to compare their performance:
\begin{itemize}
\item Embedding in packet buffer: As we already allocate 2KB chunk for each
umem packet buffer, we reserve the first 256-byte in each umem chunk as struct dp\_packet
and initialize the dp\_packet's packet independent fields  at allocation time.
This is similar to the DPDK mbuf design~\cite{dpdkmbuf}, where a single memory buffer
is used for both packet data and metadata.
\item Separate from packet buffer: This design allocates a contiguous memory
region storing an array of packet metadata, and initialize their packet
independent fields.
\end{itemize}

%complete separation of the allocation of metadata structures from the allocation of packet data buffers.
% see mbuf dpdk https://doc.dpdk.org/guides/prog_guide/mbuf_lib.html
%Embed metadata within a single memory buffer the structure followed by a fixed size area for
% the packet data.

With the above design change, we find that using the LIFO-ptr\_array in
Section~\ref{memorym} with separated packet metadata allocation for
metadata allocation yields the
best performance. It is because both data structures have better spatial
locality and they are more batching friendly.
For example, accessing 32 packet metadata in an array incurs less cache
misses than accessing the packet metadata in 32 umem chunks.
The ptr\_array for umempool management outperforms the other two designs
for the similar reason. With the above design decisions, the OVS rxdrop
can achieve similar performance as the baseline xdpsock rxdrop at around
$19Mpps$.

\subsubsection{Batching Send Syscall}
%574.790 M/sec  
With all the aforementioned optimizations, We continue measuring the
performance of OVS-AFXDP l2fwd and observe only $4Mpps$, compared to
$17Mpps$ baseline xdpsock-l2fwd.
We find that the OVS PMD thread under rxdrop has much fewer context
switches compared to the l2fwd, indicating that
the PMD process spends much more time in kernel space than in userspace.
By using strace, we find that the OVS-AFXDP l2fwd experiment calls sendto
system call at very high rate. It is because we design to check the
completion of send immediately after issusing send as shown in step 3 and
4 in Figure~\ref{afxdptx}.
We modify this design by calling sendmsg syscall (step 3) only when TX ring is
close to full, e.g., when 3/4 ring elements have been used.
For example, instead of issuing sendmsg syscall for a batch of 32 packets
and making sure they are finished, we only issue send when there are 512
outstanding packets.  With this change, the OVS-AFXDP l2fwd experiment
can achieve around $14Mpps$.

\subsubsection{Summary}
We summarize the OVS-AFXDP in Table~\ref{compare}.
Through the step-by-step analysis, the keys to performance improvement are to
keep packet processing in userspace to avoid kernel userspace context switch,
and allocating dedicated userspace processes for packet processing.
Moreover, standard optimization techniques such as batching is critical
to performance. We observe performance boost when apply batching to a couple
of places such as issuing sendmsg syscall, packet reception and transmission.
\begin{table}
\centering
\small
\begin{tabular}{|c | c | c|}
\hline
 {\bf Projects} & {\bf xdpsock } & {\bf OVS-AFXDP} \\ \hline\hline
  rxdrop & 19Mpps & 19Mpps \\ \hline
  l2fwd  & 17Mpps & 14Mpps \\ \hline
  \end{tabular}
\caption{\footnotesize
Performance comparison of the xdpsock and OVS-AFXDP.
}
\label{compare}
\end{table}

Although there are still rooms for improvement, we are now working on
making the patch upstream to the OVS code base for more people to use.

\section{Conclusion}
\label{sec:conclusion}
This paper describes two eBPF projects related to OVS: OVS-eBPF and OVS-AFXDP.
The eBPF project originally had the ambitious goal of replacing the fixed
kernel datapath with a dynamic datapath that may be injected on demand, while
keeping the design and operation essentially the same. This would solve
distribution and maintenance issues, as the version of the datapath would be
distributed with the OVS userspace package; furthermore, it would outline a
path to allow OVS to continue to process packets in the kernel without the use
of an OVS-specific kernel module, acheiving the goals of other recent
work~\cite{ovs_tc}. Collectively, this would allow the datapath logic to be
extended and modified more easily, while reducing the maintenance burden for
that code. Longer term, with a more flexible code base, many performance and
feature improvements could be made and more easily, as the constraints around
backwards compatibility of the fixed kernel ABI could be relaxed.

While the basic use cases for implementing the OpenFlow forwarding model can be
achieved in a straightforward manner in OVS-eBPF, implementing the full
capabilities of the current OVS datapath is more difficult within the bounds of
the BPF verification engine~\cite{extensible_ovs}. This paper explored an
additional approach which makes use of another datapath implementation that OVS
contains in userspace. Rather than building an extensible datapath and
inserting this logic into the kernel at runtime, the packets can be efficiently
directed to userspace for network processing using AF\_XDP sockets. The
userspace datapath implementation has none of the distribution, maintenance or
compatibility issues that the kernel implementation has, so at face value this
appears to serve many of the original goals of the OVS-eBPF project.

Over the course of exploring the use of AF\_XDP sockets, multiple design
choices were made with the goal of optimal performance in mind. Future
evaluation of this project should investigate the tradeoffs made with these
decisions, and how they affect the configuration and deployment of the
implementation. This implementation contains a lot of technical parallels with
the OVS-DPDK project, which puts certain constraints on deployment such as the
configuration of devices and the dedication of CPU and memory resources to the
datapath. In some deployment environments, these may not be feasible
constraints to place on OVS, and the existing kernel implementation does not
require such constraints.

\bibliography{paper.bib}
\bibliographystyle{plain}

\end{document}
